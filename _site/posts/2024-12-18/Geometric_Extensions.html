<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--><html class="no-js" lang="en"><!--<![endif]-->

<head>
    <meta charset="utf-8">
<title>Geometric Algorithms for Neural Combinatorial Optimization &#8211; Nikos Karalias</title>
<meta name="description" content="Balboa Station. Watch your step">
<meta name="keywords" content="Learning with constraints, Neural Networks, Combinatorial optimization">

<!-- Google Analytics-->
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-D5Q79272JD"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-D5Q79272JD');
  </script>
<!-- MathJax -->
<script type="text/javascript" async 
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>









<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Geometric Algorithms for Neural Combinatorial Optimization">
<meta property="og:description" content="Balboa Station. Watch your step">
<meta property="og:url" content="http://localhost:4000/posts/2024-12-18/Geometric_Extensions.html">
<meta property="og:site_name" content="Nikos Karalias">





<link rel="canonical" href="http://localhost:4000/posts/2024-12-18/Geometric_Extensions.html">
<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Nikos Karalias Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">



<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">   
<link rel="stylesheet" href="/assets/css/academicons.min.css">
<script src="https://kit.fontawesome.com/d3501b86e1.js" crossorigin="anonymous"></script>


    <!-- HEADER IMAGE -->
   <center>
        <span class="main-header-image">
            <a href="/"><img src="/images/header/GITS_crop.jpg"></a>
        </span>
    </center> 
    <br>
    <center>
        <span class="main-header-image">
            <h1 style="font-weight:100; font-kerning:normal"><a href="/">Nikos Karalias</a></h1>
        </span>
    </center>

    <!-- NAVIGATION -->
    <br>
    <center>
    <nav class="header-nav">
        <ul class="navigation-bar">
            <li><a href="/">HOME</a></li>
            <li><a href="/publications/">RESEARCH</a></li>
            <li><a href="/about/">ABOUT</a></li>

        </ul>
    </nav>
</center>

</head>

<!-- BODY -->
<body id="post-index">
    <!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->

    <div id="main" role="main">
        <article class="hentry">

            <div class="featured-image">
            <img src="/images/geoxtensions/GN9wide.jpg" style="width:100%">
            </div>

            <!-- MAIN -->
            <h1 class="entry-title">
                <a style= "color:rgb(255, 166, 0)" >Geometric Algorithms for Neural Combinatorial Optimization</a>
            </h1>
            <h3 class="entry-subtitle">
                <a href="http://localhost:4000" style= "color: #808080"  rel="bookmark" title="" itemprop="url"></a>
            </h3>

            <hr>

            <!-- POST CONTENT -->
            <div class="entry-content">
                <p>At NeurIPS 2025 in San Diego, we will be presenting our work on geometric algorithms for neural combinatorial optimization. I will provide a brief summary of the paper and the main idea behind it. Before I continue, it may be helpful to read this <a href="https://stalence.github.io/posts/2022-12-18/Neural_Extensions.html">blog post</a> where I discuss some of the background and motivation behind the use of extensions for combinatorial optimization with neural networks.</p>

<h2 id="learning-with-extensions-background">Learning with extensions: background</h2>
<p>First, let’s quickly summarize what an extension is and how it can be used to solve combinatorial optimization problems with neural nets. 
Assume that we have a function \(f: 2^n \rightarrow \mathbb{R}\) whose input is any subset of \(n\) items. We can view the domain of \(f\) as the space of n-dimensional binary vectors, i.e., \(f: \{0,1\}^n \rightarrow \mathbb{R}\). I’m  overloading notation here to emphasize that the <em>inputs</em> of this function are <em>discrete</em>, however the <em>outputs</em> are allowed to be <em>continuous</em>. Throughout the text I will be using notation for sets and binary vectors interchangeably for \(f\).</p>

<p>In general, many well known combinatorial optimization problems will be framed as minimization (or maximization) over a feasible (sub)set of a ground set, i.e., we are looking for a set \(S \subseteq V\) which is typically represented by a vector in \(\{0,1\}^n\) that minimizes the function \(f\). So we write</p>
<h3 id="min_s-fs--text-subject-to--s-in-omega">\(\min_{S} f(S), \; \text{ subject to } S \in \Omega\),</h3>
<p>where \(\Omega\) is the feasible set.</p>

<p>Since we have a discrete (potentially black-box) function $f$, a generally applicable strategy here would be to use reinforcement learning. For example, in a simple RL approach we would parametrize a distribution \(\mathcal{D}\) over sets with a neural network. Then we would sample from that distribution to stochastically estimate \(\mathbb{E}_{S \sim \mathcal{D}}[f(S)]\) and then update the parameters of the network with the log-derivative trick so that they optimize the \(\mathbb{E}_{S \sim \mathcal{D}}[f(S)]\).To deal with the constraints, we could customize the reward in a way that penalizes infeasible samples, or we could use a custom sampling strategy that prioritizes feasible samples.</p>

<p>Extensions provide an alternative path towards resolving this. As we saw <a href="https://stalence.github.io/posts/2022-12-18/Neural_Extensions.html">previously</a>, neural extensions essentially allow us to parametrize the distribution \(\mathcal{D}\) in a way that enables exact and differentiable evaluation of \(\mathbb{E}_{S \sim \mathcal{D}}[f(S)]\), which then allows us to backpropagate.
 So given a discrete function \(f\), its neural extension is simply a new (almost everywhere) differentiable function \(\mathfrak{F}: [0,1]^n \rightarrow \mathbb{R}\)  which extends the discrete domain of the original function to a continuous domain.</p>

<p>Different extensions can be obtained from different ways of constructing \(\mathcal{D}\). Given an input instance, suppose \(\mathbf{x} \in [0,1]^n\) is the output of the neural network on that instance. For example, the instance could be a graph and the neural network could be a Graph Neural Network. The key ingredient for constructing extension is to obtain from \(\mathbf{x}\) a distribution \(\mathcal{D}_x\) that consists of sets \(S_1, S_2, \dots, S_m\) and corresponding probabilities for each set \(p_\mathbf{x}(S_1),p_\mathbf{x}(S_2), \dots, p_\mathbf{x}(S_m)\)
such that \(\mathbf{x}= \sum_{i=1}^m p_\mathbf{x}(S_i) \mathbf{1}_{S_{i}}\).
Then the extension will be defined as</p>
<h3 id="mathfrakfmathbfx-triangleq-mathbbe_s-sim-mathcald_mathbfxfs">\(\mathfrak{F}(\mathbf{x}) \triangleq \mathbb{E}_{S \sim \mathcal{D}_\mathbf{x}}[f(S)]\).</h3>
<p>We also have the following useful expression for evaluating the extension</p>
<h3 id="mathbbe_s-sim-mathcald_mathbfxfs--sum_i1m-p_mathbfxs_i-fs_i">\(\mathbb{E}_{S \sim \mathcal{D}_\mathbf{x}}[f(S)] = \sum_{i=1}^m p_\mathbf{x}(S_i) f(S_i)\).</h3>

<h2 id="extensions-via-geometric-algorithms">Extensions via geometric algorithms</h2>
<p>From the original paper already, it was apparent that one could naturally incorporate constraints into the extensions if one could find a  way to build a distribution \(\mathcal{D}_\mathbf{x}\) such that \(S_i \in \Omega\) for all \(S_i\) in the support of  \(\mathcal{D}_\mathbf{x}\). We did provide an example for a specific constraint but what wasn’t clear is the following question: 
Is there a general strategy for constructing extensions with support only on feasible sets? The answer, for a large class of problems, is yes. We propose a general decomposition algorithm that can generate sets and corresponding probabilities. We then have the following theorem</p>

<h3 id="there-exists-a-polynomial-time-algorithm-that-for-any-well-described-polytope-mathcalp-given-by-a-strong-optimization-oracle-for-any-rational-vector-mathbfx-finds-vertex-probability-pairs-p_mathbfx_ts_t-mathbf1_s_t--for-t01-dots-n-1--such-that--mathbfx-sum_t0n-1-p_mathbfx_ts_t-mathbf1_s_t-and-all-p_mathbfx_ts_t-are-almost-everywhere-differentiable-functions-of-mathbfx">There exists a polynomial-time algorithm that for any well-described polytope \(\mathcal{P}\) given by a strong optimization oracle, for any rational vector \(\mathbf{x}\), finds vertex-probability pairs \(p_{\mathbf{x}_t}(S_t), \mathbf{1}_{S_t}\)  for \(t=0,1, \dots, n-1\)  such that  \(\mathbf{x}= \sum_{t=0}^{n-1} p_{\mathbf{x}_{t}}(S_t) \mathbf{1}_{S_t}\) and all \(p_{\mathbf{x}_t}(S_t)\) are almost everywhere differentiable functions of \(\mathbf{x}\).</h3>

<p>The theorem relies on an algorithmic version of the Carathéodory theorem by Martin Grötschel, László Lovász, and Alexander Schrijver (GLS decomposition), that appears in their book “Geometric Algorithms and Combinatorial Optimization”.</p>

<p>Briefly, we first define a polytope whose corners are all the feasible sets \(S \in \Omega\). Then, if there is an oracle for linear optimization on the polytope, such an extension can be constructed. To do so, given any interior point \(\mathbf{x}\) in the polytope, we algorithmically construct its Carathéodory decomposition as follows.</p>

<ol>
  <li>Set \(\mathbf{1}_{S_i} \triangleq \text{argmax}_{\mathbf{c \in \mathcal{P}}}\mathbf{c}^\top \mathbf{x}\)</li>
  <li>Fire a ray starting from \(\mathbf{1}_{S_i}\) that passes through \(\mathbf{x}\).</li>
  <li>Compute its intersection \(\mathbf{x'}\) with the boundary of the polytope.</li>
  <li>Clearly, \(\mathbf{x}\) is a point in the line segment whose endpoints are \(\mathbf{1}_{S_i}\) and \(\mathbf{x}'\). It is therefore a convex combination of the two.</li>
  <li>Set \(\mathbf{x} \leftarrow \mathbf{x'}\) and repeat from step 1.</li>
</ol>

<p>By the GLS decomposition, this process will terminate in at most \(n+1\) steps. A key part of the theorem is to show that that the coefficients forming the convex combinations at each iteration of the GLS decomposition are almost differentiable functions of \(\mathbf{x}\) at each step, which allows us to backpropagate through the extension.</p>

<p>Our theorem is applicable to any feasible set polytope that admits an efficient linear optimization oracle.
 This includes optimization problems cardinality constraints, matroid constraints, and permutations. In the paper we provide worked out examples for the decompositions of cardinality constraints, spanning trees, and independent sets. The independent set polytope does not admit polytime oracle for linear optimization but we discuss how one could work around this issue in cases where such an oracle is not available.</p>

<p>It is important to note that hard combinatorial problems can often be reformulated in terms of polytopes that admit linear optimization oracles. For example, the Travelling Salesperson Problem (TSP) can be viewed as a linear optimization problem over the TSP polytope but it can also be viewed as quadratic optimization over a permutation polytope. In that case, the TSP problem would be solved by producing an extension of the quadratic objective over the feasible set of permutations. This significantly improves the applicability of our approach.</p>

<p>Finally, our proposed general decomposition algorithm is more general than the GLS decomposition since it allows for approximate versions of the Carathéodory decomposition. We provide an example of such a decomposition which we were able to use in our experiments to improve results on the maximum coverage problem.</p>


            </div>

            <!--- DIVIDING LINE -->
            <hr>

            <!-- POST TAGS -->
            <div class="inline-tags">
                <span>
                    
                        <a href="/tags/#Learning with constraints">#Learning with constraints&nbsp;&nbsp;&nbsp;</a>
                    
                        <a href="/tags/#Neural Networks">#Neural Networks&nbsp;&nbsp;&nbsp;</a>
                    
                        <a href="/tags/#Combinatorial optimization">#Combinatorial optimization&nbsp;&nbsp;&nbsp;</a>
                    
                </span>
            </div>

            <br>

            <!-- POST DATE -->
            <div class="post-date">
                November 1, 2025
            </div>
        </article>
    </div>
</body>

<!-- FOOTER -->
<footer>
    <div class="footer-wrapper">
        <footer role="contentinfo">
            <span>
    &copy; 2025 Nikolaos Karalias.<br>Powered by <a target="_blank" href="https://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a target="_blank" href="https://github.com/benradford/Slate-and-Simple-Jekyll-Theme">Slate+Simple</a> theme.
</span>

        </footer>
    </div>
</footer>
</html>
