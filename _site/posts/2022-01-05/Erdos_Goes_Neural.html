<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--><html class="no-js" lang="en"><!--<![endif]-->

<head>
    <meta charset="utf-8">
<title>Erdos goes neural &#8211; Nikos Karalias</title>
<meta name="description" content="Balboa Station. Watch your step">
<meta name="keywords" content="Erdős goes neural, Combinatorial Optimization, Neural Networks">

<!-- Google Analytics-->


<!-- MathJax -->
<script type="text/javascript" async 
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
</script>

<script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>









<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Erdos goes neural">
<meta property="og:description" content="Balboa Station. Watch your step">
<meta property="og:url" content="http://localhost:4000/posts/2022-01-05/Erdos_Goes_Neural.html">
<meta property="og:site_name" content="Nikos Karalias">





<link rel="canonical" href="http://localhost:4000/posts/2022-01-05/Erdos_Goes_Neural.html">
<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Nikos Karalias Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">



<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">   
<link rel="stylesheet" href="/assets/css/academicons.min.css">
<script src="https://kit.fontawesome.com/d3501b86e1.js" crossorigin="anonymous"></script>


    <!-- HEADER IMAGE -->
   <center>
        <span class="main-header-image">
            <a href="/"><img src="/images/header/GITS_crop.jpg"></a>
        </span>
    </center> 
    <br>
    <center>
        <span class="main-header-image">
            <h1 style="font-weight:100; font-kerning:normal"><a href="/">Nikos Karalias</a></h1>
        </span>
    </center>

    <!-- NAVIGATION -->
    <br>
    <center>
    <nav class="header-nav">
        <ul class="navigation-bar">
            <li><a href="/">HOME</a></li>
            <li><a href="/publications/">RESEARCH</a></li>
            <li><a href="/about/">ABOUT</a></li>

        </ul>
    </nav>
</center>

</head>

<!-- BODY -->
<body id="post-index">
    <!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->

    <div id="main" role="main">
        <article class="hentry">

            <div class="featured-image">
            <img src="/images/erdos_neural/erdos6.jpg" style="width:100%">
            </div>

            <!-- MAIN -->
            <h1 class="entry-title">
                <a style= "color:rgb(255, 166, 0)" >Erdos goes neural</a>
            </h1>
            <h3 class="entry-subtitle">
                <a href="http://localhost:4000" style= "color: #808080"  rel="bookmark" title="" itemprop="url">The probabilistic method for neural combinatorial optimization.</a>
            </h3>

            <hr>

            <!-- POST CONTENT -->
            <div class="entry-content">
                <h3 id="disclaimer">Disclaimer:</h3>
<p>This is a brief tutorial on the probabilistic penalty method presented in our <a href="https://arxiv.org/abs/2006.10643">paper</a>. I will be taking shortcuts in how I present it in order to make it straightforward and understandable. It does not cover the paper in its full generality. I will not explain the probabilistic method and I won’t expand on the various important details that constitute the full paper.
Instead, I aim to provide an accessible primer into our work by minimizing the technical info and focusing on practicality/simplicity. I encourage you to read the paper and the supplementary material section for the complete treatment. Otherwise, if you’re still unsure, just email me. Without further ado, let’s get started.</p>

<h2 id="erdos-goes-neural-a-simplified-tutorial">Erdos goes neural: a simplified tutorial</h2>

<h3 id="preliminaries">Preliminaries:</h3>
<p>Before we start, we should clear up what the goal of this work is. The main objective is to solve combinatorial problems with neural networks, 
<strong>without supervision</strong> (i.e., without access to labeled solutions). We focused on graph problems but the principles of our method can be applied to other settings. Furthermore, many (combinatorial) problems have an equivalent graph formulation, so you could reformulate your problem in a graph setting if you want to be as close to what we did as possible.</p>

<p>OK, great. Now that this is out of the way, let’s talk about how we are going to get things done.
First, let’s be a bit more specific with the setup.</p>

<p>We want a set of objects \(S\) that solves a given problem, e.g., finds the set of nodes that forms the largest possible clique on a graph (maximum clique problem). 
Since we focused on graph problems in the paper, we will work with sets of nodes on the input graph, but you can also do sets of edges/tuples/etc. It will depend on your particular problem and what is more convenient for you. 
<strong>From now on I will be talking about sets of nodes. Feel free to substitute it with edges or whatever else you need to work with.</strong></p>

<p>Here is what we are going to need:</p>
<h4 id="1-a-combinatorial-problem-with-a-nonnegative-cost-function-and-a-set-of-constraints">1) A combinatorial problem with a nonnegative cost function and a set of constraints.</h4>
<h4 id="2-a-neural-network-that-produces-a-probability-for-each-node-this-is-the-probability-that-the-node-belongs-to-the-set-s">2) A neural network that produces a probability for each node. This is the probability that the node belongs to the set \(S\).</h4>
<h4 id="3-a-differentiable-loss-function-this-loss-takes-as-input-the-probabilities-that-were-produced-by-the-network-the-loss-will-be-derived-from-your-problems-objective-i-will-explain-below">3) A differentiable loss function. This loss takes as input the probabilities that were produced by the network. The loss will be derived from your problem’s objective (I will explain below).</h4>

<h4 id="technical-detail">Technical Detail:</h4>
<p>In the paper, we use Bernoulli variables over the nodes of the input graph. You could work with other distributions as well, but if you are unsure I recommend starting with Bernoulli  random variables over the entities you care about (over nodes/edges/tuples/etc.). We will need to derive an expectation of a function over those random variables and it happens that Bernoulli variables tend to lead to easier derivations.
For the rest of the post \(x_i\) is a Bernoulli random variable placed on node \(i\),</p>

\[x_i= \begin{cases} 1, \quad \text{with probability } p_i  \\
0, \quad \text{with probability } 1-p_i .
\end{cases}\]

<h2 id="how-to-solve-it">How to solve it</h2>

<p>Now, suppose you have a combinatorial problem. You need to follow the steps below.</p>
<h3 id="step-1">Step 1:</h3>
<p>Write down the cost function of your problem, for example in our paper we consider the maximum clique problem.
Here is the standard way of expressing it:</p>
<h4 id="maximize-weights-subject-to-s-is-a-clique">maximize weight(\(S\)), subject to: \(S\) is a clique.</h4>
<p>For a simple undirected graph, weight(S) (henceforth \(w(S)\)) just counts the number of edges in the subgraph induced by S. 
To be in line with the paper, we need to switch to a  minimization problem, hence:</p>
<h4 id="minimize-gamma-ws-subject-to-s-is-a-clique">minimize \(\gamma-w(S)\), subject to: \(S\) is a clique.</h4>
<p>\(\gamma\) here is a sufficiently large constant;  Let \(\gamma \geq \text{max}_{S}(w(S))\), so that the expression remains always nonnegative.</p>
<hr />

<h3 id="step-2">Step 2:</h3>
<p>Set up a graph neural network (the choice of model depends on the task and engineering considerations) for your data. Using any of the mainstream
GNNs (GIN, GAT, etc.) should be fine to start with.
The GNN takes as input some node features. Its output is an N x 1 vector of probabilities, one for each node.
For the specifics on features, layers, normalizations, etc. you can just look at the code in the repo. Or you may improve upon the pipeline by working with your own features, layers, etc.</p>
<hr />

<h3 id="step-3">Step 3:</h3>
<p>Derive a differentiable loss.
The differentiable loss function has to look like this:</p>
<h4 id="loss--expected-cost--beta--ps-does-not-satisfy-constraints">Loss = Expected Cost + \(\beta\) * P(S does not satisfy constraints).</h4>
<p>\(\beta\) here is a coefficient that controls the importance of the constraint in the loss. 
The expectation of the cost can be straightforward for many set functions. 
For the Prob(S does not satisfy constraints)  term (henceforth \(P(S \notin \Omega)\)), we can use Markov’s inequality to bound it.</p>

<h3 id="example">Example:</h3>
<p>First, derive the expected cost. In our max-clique example, \(\text{cost} = \gamma-w(S)\).
\(\gamma\) is just a constant so \(E[cost] = \gamma - E[w(S)]\).
We have \(w(S) = \sum_{(i,j) \in E}x_i x_j\) where \(E\) is the set of edges of the graph.</p>

<p>In other words, an edge \((i,j)\) is in  \(S\) if both endpoints \(i\) and \(j\) are in  \(S\).
We are using Bernoulli variables, therefore:
\(E[w(S)] = \sum_{(i,j)\in E}p_i p_j.\)</p>

<p>OK, now onto the constraint part of the loss.
For the maximum clique problem, the constraint dictates that <strong>the subgraph induced by S is a clique</strong>, i.e, all pairs of nodes in S are connected by an edge.
An equivalent way to phrase this is that, <strong>there are no edges in the complement of the subgraph induced by S</strong>.
Let \(\bar{S}\) be the complement of S. Then,</p>
<h4 id="ps-notin-omega---pwbars1">\(P(S \notin \Omega)  = P(w(\bar{S})&gt;=1).\)</h4>
<p>From Markov’s inequality, this can be bounded as follows</p>
<h4 id="pwbars1-leq-ewbars">\(P(w(\bar{S})&gt;=1) \leq E[w(\bar{S})].\)</h4>
<p>So our loss ends up being</p>
<h4 id="textloss--gamma---ews--beta-ewbars">\(\text{Loss} = \gamma - E[w(S)] + \beta E[w(\bar{S})].\)</h4>

<hr />

<h3 id="step-4">Step 4:</h3>
<p>Train the network using the derived loss. This is straightforward because you just have to plug in the probabilities in the expression and do backprop.</p>
<hr />

<h3 id="step-5">Step 5:</h3>
<p>Retrieve the set \(S\) from the probabilities of the network using the method of conditional expectation.
It works as follows.</p>

<p>Sort the nodes according to their probabilities.
Starting from the high probability nodes, for each node \(v_i\) do:</p>

<ol>
  <li>Evaluate the loss for \(p_i=1\) and for \(p_i=0\).</li>
  <li>Set  \(p_i\) to either 1 or 0 depending on what achieved the better loss.</li>
  <li>Move on to the next node and repeat from step 1.</li>
</ol>

<p>When this is done, you should have a binary (indicator) vector that represents your set \(S\), which is the solution to your problem.
Congrats! That’s it, you’re done.</p>


            </div>

            <!--- DIVIDING LINE -->
            <hr>

            <!-- POST TAGS -->
            <div class="inline-tags">
                <span>
                    
                        <a href="/tags/#Erdős goes neural, Combinatorial Optimization, Neural Networks">#Erdős goes neural, Combinatorial Optimization, Neural Networks&nbsp;&nbsp;&nbsp;</a>
                    
                </span>
            </div>

            <br>

            <!-- POST DATE -->
            <div class="post-date">
                January 5, 2022
            </div>
        </article>
    </div>
</body>

<!-- FOOTER -->
<footer>
    <div class="footer-wrapper">
        <footer role="contentinfo">
            <span>
    &copy; 2022 Nikolaos Karalias.<br>Powered by <a target="_blank" href="https://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a target="_blank" href="https://github.com/benradford/Slate-and-Simple-Jekyll-Theme">Slate+Simple</a> theme.
</span>

        </footer>
    </div>
</footer>
</html>
