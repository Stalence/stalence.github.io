<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2026-02-24T11:25:51-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Nikos Karalias</title><subtitle>Balboa Station. Watch your step</subtitle><entry><title type="html">Geometric Algorithms for Neural Combinatorial Optimization</title><link href="http://localhost:4000/posts/2025-10-30/Geometric_Extensions.html" rel="alternate" type="text/html" title="Geometric Algorithms for Neural Combinatorial Optimization" /><published>2025-10-30T00:00:00-04:00</published><updated>2025-10-30T00:00:00-04:00</updated><id>http://localhost:4000/posts/2025-10-30/Geometric_Extensions</id><content type="html" xml:base="http://localhost:4000/posts/2025-10-30/Geometric_Extensions.html">&lt;p&gt;At NeurIPS 2025 in San Diego, we will be presenting our &lt;a href=&quot;https://arxiv.org/pdf/2510.24039&quot;&gt;paper on geometric algorithms for neural combinatorial optimization&lt;/a&gt;. I will provide a brief summary of the paper and the main idea behind it. Before I continue, it may be helpful to read this &lt;a href=&quot;https://stalence.github.io/posts/2022-12-18/Neural_Extensions.html&quot;&gt;blog post&lt;/a&gt; where I discuss some of the background and motivation behind the use of extensions for combinatorial optimization with neural networks.&lt;/p&gt;

&lt;h2 id=&quot;learning-with-extensions-background&quot;&gt;Learning with extensions: background&lt;/h2&gt;
&lt;p&gt;First, let’s quickly summarize what an extension is and how it can be used to solve combinatorial optimization problems with neural nets. 
Assume that we have a function \(f: 2^n \rightarrow \mathbb{R}\) whose input is any subset of \(n\) items. We can view the domain of \(f\) as the space of n-dimensional binary vectors, i.e., \(f: \{0,1\}^n \rightarrow \mathbb{R}\). I’m  overloading notation here to emphasize that the &lt;em&gt;inputs&lt;/em&gt; of this function are &lt;em&gt;discrete&lt;/em&gt;, however the &lt;em&gt;outputs&lt;/em&gt; are allowed to be &lt;em&gt;continuous&lt;/em&gt;. Throughout the text I will be using notation for sets and binary vectors interchangeably for \(f\).&lt;/p&gt;

&lt;p&gt;In general, many well known combinatorial optimization problems will be framed as minimization (or maximization) over a feasible (sub)set of a ground set, i.e., we are looking for a set \(S \subseteq V\) which is typically represented by a vector  \(\mathbf{1}_{S} \in \{0,1\}^n\) that minimizes the function \(f\). So we write&lt;/p&gt;
&lt;h3 id=&quot;min_s-fs--text-subject-to--s-in-omega&quot;&gt;\(\min_{S} f(S), \; \text{ subject to } S \in \Omega\),&lt;/h3&gt;
&lt;p&gt;where \(\Omega\) is the feasible set.&lt;/p&gt;

&lt;p&gt;Since we have a discrete (potentially black-box) function \(f\), a generally applicable strategy here would be to use reinforcement learning. For example, in a simple RL approach we would parametrize a distribution \(\mathcal{D}\) over sets with a neural network. Then we would sample from that distribution to stochastically estimate \(\mathbb{E}_{S \sim \mathcal{D}}[f(S)]\) and then update the parameters of the network with the log-derivative trick so that they optimize the \(\mathbb{E}_{S \sim \mathcal{D}}[f(S)]\). To deal with the constraints, we could customize the reward in a way that penalizes infeasible samples, or we could use a custom sampling strategy that prioritizes feasible samples.&lt;/p&gt;

&lt;p&gt;Extensions provide an alternative path towards resolving this. As we saw &lt;a href=&quot;https://stalence.github.io/posts/2022-12-18/Neural_Extensions.html&quot;&gt;previously&lt;/a&gt;, neural extensions essentially allow us to parametrize the distribution \(\mathcal{D}\) in a way that enables exact and differentiable evaluation of \(\mathbb{E}_{S \sim \mathcal{D}}[f(S)]\), which then allows us to backpropagate.
 So given a discrete function \(f\), its neural extension is simply a new (almost everywhere) differentiable function \(\mathfrak{F}: [0,1]^n \rightarrow \mathbb{R}\)  which extends the discrete domain of the original function to a continuous domain.&lt;/p&gt;

&lt;p&gt;Different extensions can be obtained from different ways of constructing \(\mathcal{D}\). Given an input instance, suppose \(\mathbf{x} \in [0,1]^n\) is the output of the neural network on that instance. For example, the instance could be a graph and the neural network could be a Graph Neural Network. The key ingredient for constructing extension is to obtain from \(\mathbf{x}\) a distribution \(\mathcal{D}_x\) that consists of sets \(S_1, S_2, \dots, S_m\) and corresponding probabilities for each set \(p_\mathbf{x}(S_1),p_\mathbf{x}(S_2), \dots, p_\mathbf{x}(S_m)\)
such that \(\mathbf{x}= \sum_{i=1}^m p_\mathbf{x}(S_i) \mathbf{1}_{S_{i}}\).
Then the extension will be defined as&lt;/p&gt;
&lt;h3 id=&quot;mathfrakfmathbfx-triangleq-mathbbe_s-sim-mathcald_mathbfxfs&quot;&gt;\(\mathfrak{F}(\mathbf{x}) \triangleq \mathbb{E}_{S \sim \mathcal{D}_\mathbf{x}}[f(S)]\).&lt;/h3&gt;
&lt;p&gt;We also have the following useful expression for evaluating the extension&lt;/p&gt;
&lt;h3 id=&quot;mathbbe_s-sim-mathcald_mathbfxfs--sum_i1m-p_mathbfxs_i-fs_i&quot;&gt;\(\mathbb{E}_{S \sim \mathcal{D}_\mathbf{x}}[f(S)] = \sum_{i=1}^m p_\mathbf{x}(S_i) f(S_i)\).&lt;/h3&gt;

&lt;h2 id=&quot;extensions-via-geometric-algorithms&quot;&gt;Extensions via geometric algorithms&lt;/h2&gt;
&lt;p&gt;From the original paper already, it was apparent that one could naturally incorporate constraints into the extensions if one could find a  way to build a distribution \(\mathcal{D}_\mathbf{x}\) such that \(S_i \in \Omega\) for all \(S_i\) in the support of  \(\mathcal{D}_\mathbf{x}\). We did provide an example for a specific constraint but what wasn’t clear is the following question: 
Is there a general strategy for constructing extensions with support only on feasible sets? The answer, for a large class of problems, is yes. We propose a general decomposition algorithm that can generate sets and corresponding probabilities. We then have the following theorem:&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:rgb(255, 166, 0)&quot;&gt; There exists a polynomial-time algorithm that for any well-described polytope \(\mathcal{P}\) given by a strong optimization oracle, for any rational vector \(\mathbf{x}\), finds vertex-probability pairs \(p_{\mathbf{x}_t}(S_t), \mathbf{1}_{S_t}\)  for \(t=0,1, \dots, n-1\)  such that  \(\mathbf{x}= \sum_{t=0}^{n-1} p_{\mathbf{x}_{t}}(S_t) \mathbf{1}_{S_t}\) and all \(p_{\mathbf{x}_t}(S_t)\) are almost everywhere differentiable functions of \(\mathbf{x}\). &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The theorem relies on an algorithmic version of the Carathéodory theorem by Martin Grötschel, László Lovász, and Alexander Schrijver (GLS decomposition), that appears in their book &lt;span style=&quot;color:rgb(255, 166, 0)&quot;&gt;“Geometric Algorithms and Combinatorial Optimization”.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Briefly, we first define a polytope whose corners are all the feasible sets \(S \in \Omega\). Then, if there is an oracle for linear optimization on the polytope, such an extension can be constructed. To do so, given any interior point \(\mathbf{x}\) in the polytope, we algorithmically construct its Carathéodory decomposition as follows.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Set \(\mathbf{1}_{S_i} \leftarrow \text{argmax}_{\mathbf{c \in \mathcal{P}}}\mathbf{c}^\top \mathbf{x}\)&lt;/li&gt;
  &lt;li&gt;Fire a ray starting from \(\mathbf{1}_{S_i}\) that passes through \(\mathbf{x}\).&lt;/li&gt;
  &lt;li&gt;Compute its intersection \(\mathbf{x&apos;}\) with the boundary of the polytope.&lt;/li&gt;
  &lt;li&gt;Clearly, \(\mathbf{x}\) is a point in the line segment whose endpoints are \(\mathbf{1}_{S_i}\) and \(\mathbf{x}&apos;\). It is therefore a convex combination of the two.&lt;/li&gt;
  &lt;li&gt;Set \(\mathbf{x} \leftarrow \mathbf{x&apos;}\) and repeat from step 1.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;By the GLS decomposition, this process will terminate in at most \(n+1\) steps. A key part of the theorem is to show that that the coefficients forming the convex combinations at each iteration of the GLS decomposition are (almost everywhere) differentiable functions of \(\mathbf{x}\) at each step, which allows us to backpropagate through the extension.&lt;/p&gt;

&lt;p&gt;Our theorem is applicable to any feasible set polytope that admits an efficient linear optimization oracle.
 This includes optimization problems cardinality constraints, matroid constraints, and permutations. In the paper we provide worked out examples for the decompositions of cardinality constraints, spanning trees, and independent sets. The independent set polytope does not admit a polytime oracle for linear optimization but we discuss how one could work around this issue in cases where such an oracle is not available.&lt;/p&gt;

&lt;p&gt;It is important to note that &lt;span style=&quot;color:rgb(255, 166, 0)&quot;&gt; hard combinatorial problems can often be reformulated in terms of polytopes that admit linear optimization oracles &lt;/span&gt;. For example, the Travelling Salesperson Problem (TSP) can be viewed as a linear optimization problem over the TSP polytope but it can also be viewed as quadratic optimization over a permutation polytope. That permutation polytope (Birkhoff Polytope) admits a fast oracle for its linear optimization problem. In that case, the TSP problem would be solved by producing an extension of the quadratic objective over the feasible set of permutations. This significantly improves the applicability of our approach.&lt;/p&gt;

&lt;p&gt;Finally, our proposed decomposition algorithm is more general than the GLS decomposition since &lt;span style=&quot;color:rgb(255, 166, 0)&quot;&gt;it allows for approximate versions of the Carathéodory decomposition (Proposition 4.2 in the paper) &lt;/span&gt;. We provide an example of such a decomposition which we were able to use in our experiments to improve results on the maximum coverage problem.&lt;/p&gt;</content><author><name></name></author><category term="posts" /><category term="Learning with constraints" /><category term="Neural Networks" /><category term="Combinatorial optimization" /><summary type="html">At NeurIPS 2025 in San Diego, we will be presenting our paper on geometric algorithms for neural combinatorial optimization. I will provide a brief summary of the paper and the main idea behind it. Before I continue, it may be helpful to read this blog post where I discuss some of the background and motivation behind the use of extensions for combinatorial optimization with neural networks.</summary></entry><entry><title type="html">Are Neural Networks Optimal Approximation Algorithms?</title><link href="http://localhost:4000/posts/2024-12-18/Csailnews.html" rel="alternate" type="text/html" title="Are Neural Networks Optimal Approximation Algorithms?" /><published>2024-12-18T00:00:00-05:00</published><updated>2024-12-18T00:00:00-05:00</updated><id>http://localhost:4000/posts/2024-12-18/Csailnews</id><content type="html" xml:base="http://localhost:4000/posts/2024-12-18/Csailnews.html">&lt;p&gt;This NeurIPS, we presented our work on the question of whether neural networks can be efficiently used to design optimal approximation algorithms. The answer is a conditional yes. Assuming the Unique Games Conjecture, there is a general
semidefinite program (SDP) that achieves optimal approximation guarantees for all maximum constraint satisfaction problems. In our paper, we show that this SDP can be efficiently solved using a rather simple graph neural network architecture (GNN). More specifically, gradient descent on the quadratically penalized Lagrangian of the SDP leads to message passing iterations, which we execute through a neural network that we call OptGNN.&lt;/p&gt;

&lt;p&gt;This leads to an architecture that achieves strong empirical results across several NP-Hard combinatorial problems. The paper has also been featured on &lt;a href=&quot;https://www.csail.mit.edu/news/deep-learning-np-hard-problems&quot;&gt;MIT CSAIL news&lt;/a&gt; so feel free to check that out for some additional commentary.&lt;/p&gt;</content><author><name></name></author><category term="posts" /><category term="Learning with approximation guarantees" /><category term="Neural Networks" /><category term="Combinatorial optimization" /><summary type="html">This NeurIPS, we presented our work on the question of whether neural networks can be efficiently used to design optimal approximation algorithms. The answer is a conditional yes. Assuming the Unique Games Conjecture, there is a general semidefinite program (SDP) that achieves optimal approximation guarantees for all maximum constraint satisfaction problems. In our paper, we show that this SDP can be efficiently solved using a rather simple graph neural network architecture (GNN). More specifically, gradient descent on the quadratically penalized Lagrangian of the SDP leads to message passing iterations, which we execute through a neural network that we call OptGNN.</summary></entry><entry><title type="html">Learning with Neural Extensions</title><link href="http://localhost:4000/posts/2022-12-18/Neural_Extensions.html" rel="alternate" type="text/html" title="Learning with Neural Extensions" /><published>2022-12-18T00:00:00-05:00</published><updated>2022-12-18T00:00:00-05:00</updated><id>http://localhost:4000/posts/2022-12-18/Neural_Extensions</id><content type="html" xml:base="http://localhost:4000/posts/2022-12-18/Neural_Extensions.html">&lt;p&gt;This post is meant to serve as an intuitive and practical explainer of our work on &lt;a href=&quot;https://arxiv.org/abs/2208.04055&quot;&gt;neural set function extensions&lt;/a&gt;, recently presented at NeurIPS 2022. As in previous posts, I will be skipping potentially important details in the interest of brevity and accessibility. Throughout the text, I will be making an effort to accommodate the non-expert reader. This might make the text a bit tedious or repetitive for the people already familiar with this kind of setting, so feel free to skim through the boring parts if that’s you. Now, let’s get started.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Neural networks natively operate in continuous domains, where we can update their parameters through gradient-based optimization. However, many problems that we care about solving are defined over discrete domains and their solutions often involve different kinds of discrete computation. These may be algorithmic tasks like sorting numbers or finding shortest paths on graphs, or maybe playing games like chess and Go. Applying neural networks to discrete domains/tasks can lead to the discovery of new effective algorithms for those problems but at the same time it comes with its own set of challenges. Discrete representations and operations introduce discontinuities and non-differentiabilities in end to end pipelines. Our paper is another step towards overcoming some of the incompatibilities between neural networks and discrete computation.&lt;/p&gt;

&lt;p&gt;Zooming in a bit more, a general problem that is commonly found in machine learning is that of picking an object or a set of objects that is the best with respect to a certain criterion. For example, in image classification, we predict some labels and the criterion we care about is whether the predicted labels match the true labels of the data. In strategy games, we often have a discrete space of actions and the criterion is some reward defined over the action space. In combinatorial optimization, we have a discrete space of possible configurations of objects and we are interested in the configuration that minimizes an objective function. In all of those cases, there is a function defined on a discrete space (space of labels, space of actions, space of configurations) which evaluates the quality of the &lt;em&gt;discrete input&lt;/em&gt; that is provided. However, neural networks produce continuous representations which are incompatible with such functions. Furthermore, such discrete functions aren’t necessarily differentiable either, so we can’t directly use them as the loss function of our neural network. To solve such problems with NNs, we use continuous and differentiable proxies in place of the original function as a training loss. In classification, we use the cross-entropy as a loss function, which acts as a proxy for the training accuracy. If we want to evaluate how well we are doing in terms of accuracy in a classification task, we may perform a discretization step (e.g., argmax) of the continuous output. If there isn’t a known differentiable proxy for the function we care about, we may treat the function as a black-box and optimize it directly using reinforcement learning algorithms. In that case, discretization happens generally by sampling discrete representations from the continuous output of the neural network.&lt;/p&gt;

&lt;p&gt;In our paper on set function extensions we propose an alternative that deals both discrete domain of functions as well as their differentiability. We show how to create &lt;span style=&quot;color:rgb(255, 166, 0)&quot;&gt;continuous and differentiable&lt;/span&gt; extensions (proxies) of discrete functions, i.e., functions compatible with continuous inputs (e.g., outputs of neural networks) that we can backpropagate through in neural network pipelines. In fact, many of our extensions 
 enable this in a completely &lt;span style=&quot;color:rgb(255, 166, 0)&quot;&gt;black-box&lt;/span&gt; manner, i.e., without requiring a closed form expression for the discrete function. Furthermore, drawing inspiration from &lt;a href=&quot;https://en.wikipedia.org/wiki/Semidefinite_programming&quot;&gt;semidefinite programming&lt;/a&gt; as well as more recent work in the ML community on neural algorithmic reasoning and knowledge graph reasoning, we show how to extend discrete domains to &lt;span style=&quot;color:rgb(255, 166, 0)&quot;&gt;higher dimensions&lt;/span&gt; so that discrete functions can be applied directly to high-dimensional embeddings. This post will focus on the low-dimensional case; extensions onto high-dimensional spaces will be discussed in part 2 of this post.&lt;/p&gt;

&lt;h2 id=&quot;a-more-concrete-example-and-possible-approaches&quot;&gt;A more concrete example and possible approaches&lt;/h2&gt;
&lt;p&gt;Suppose that you have a neural network which produces a score vector \(\mathbf{x} \in [0,1]^{n}\) given some input problem instance (a graph, an image, etc.), which is then used to solve some downstream task. That vector could be marginal probabilities of graph nodes for some kind of node selection task, or even the class probabilities for a classification task. To make the presentation more concrete, we will treat \(\mathbf{x}\) as marginal probabilities for node selection, i.e., \(x_i\) is the probability that node \(i\) belongs to a set of nodes \(S\) that is meant to solve some graph problem (a set that optimizes some kind of objective function).&lt;/p&gt;

&lt;p&gt;Now assume that you have a function \(f: 2^n \rightarrow \mathbb{R}\) whose input is any subset of \(n\) items. We can view the domain of \(f\) as the space of n-dimensional binary vectors, i.e., \(f: \{0,1\}^n \rightarrow \mathbb{R}\). I’m obviously abusing notation here to emphasize that the &lt;em&gt;inputs&lt;/em&gt; of this function are &lt;em&gt;discrete&lt;/em&gt;, however the &lt;em&gt;outputs&lt;/em&gt; are allowed to be &lt;em&gt;continuous&lt;/em&gt;. Throughout the text I will be using notation for sets and binary vectors interchangeably. 
The input domain consists of all the possible indicator vectors for all the subsets of \(n\) items we could pick. For instance, let’s assume \(f\) is the cardinality function, i.e., the function that counts how many items the provided set has. Then for \(n=3\), a set \(S\) could be represented by a 3-dimensional binary vector, e.g., \(\mathbf{1}_S= \begin{bmatrix}1 &amp;amp; 1 &amp;amp; 0 \end{bmatrix}\). In the case of that example vector, clearly the cardinality is \(f(S) = f(\mathbf{1}_S) = 2\).&lt;/p&gt;

&lt;p&gt;Naturally, that kind of function is not compatible with arbitrary \(\mathbf{x}\) due to the continuity of \(\mathbf{x}\). For instance, it doesn’t make much sense to ask what is the cardinality of \(\begin{bmatrix} 0.3 &amp;amp; 0.5 &amp;amp; 0.2 \end{bmatrix}\) to begin with.  If we want to find a set that minimizes (or maximizes) the function by improving the scores in \(\mathbf{x}\) in a differentiable neural network pipeline, we have to square away this incompatibility.
Here, there are certain options one would consider:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Discretize the continuous output. We could sample with the Gumbel trick or even just threshold the values of \(\mathbf{x}\) to obtain a binary vector that represents a set. We could then use a &lt;a href=&quot;https://www.hassanaskary.com/python/pytorch/deep%20learning/2020/09/19/intuitive-explanation-of-straight-through-estimators.html#what-is-a-straight-through-estimator&quot;&gt;straight-through estimator&lt;/a&gt; in the backward pass to go through the discretization procedure. While this provides a discrete vector that \(f\) is compatible with, it may only work if the function \(f\) itself &lt;em&gt;is differentiable&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There will be plenty of cases where \(f\) is given as a black box, so we have no guarantees that we can differentiate through it. In that case, some kind of &lt;a href=&quot;https://arxiv.org/abs/1711.00123&quot;&gt;stochastic gradient estimation&lt;/a&gt; might be our next option. For example we could use REINFORCE, i.e., sample sets \(S\) from \(\mathbf{x}\) then use the log-derivative trick to backpropagate through \(\mathbf{x}\) while treating \(f\) as the reward function.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use a known continuous relaxation of the function. Again, that assumes that we have access to the function and a bespoke relaxation exists. That does not 
require any discretization as the relaxation by definition accepts continuous inputs. When available, this can be a pretty good option. In the case of the cardinality function, one has to add all the elements of the binary vector to obtain the cardinality of the set. In other words, \(f(\mathbf{1}_S)= \sum_{i=1}^n \mathbb{I}_{i \in S}\). Here, \(\mathbb{I}_{i \in S}\) returns 1 if \(i \in S\) and 0 otherwise. This can be naturally relaxed in the \([0,1]^n\) interval by taking \(\bar{f}(\mathbf{x}) = \sum_{i=1}^n x_i.\)  This is great but it can only be done if we have a closed form expression for the function. In the case where the function is a black-box, we’re out of luck.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;our-solution&quot;&gt;Our solution&lt;/h2&gt;
&lt;p&gt;These are all valid strategies that could make sense in certain scenarios. Here, we will propose an alternative that aims to mitigate the drawbacks of the aforementioned apporoaches.
First, we can go back to the main obstacle we started from. What does it mean to ask about the cardinality of the input (value of the discrete function) when the input is not a set (binary vector) but a vector of continuous values?  There is a way to make this question well-posed. We can treat the continuous values as parameters of a probability distribution over sets. Then we can ask what is the &lt;em&gt;expected value of the function&lt;/em&gt; over the distribution of sets encoded by \(\mathbf{x}\).&lt;/p&gt;
&lt;h3 id=&quot;what-is-a-scalar-set-function-extension-it-is-simply-a-new-differentiable-function-mathfrakf-01n-rightarrow-mathbbr--which-extends-the-discrete-domain-of-the-original-function-to-a-continuous-domain-we-will-define-it-as-a-weighted-combination-of-evaluations-of-the-original-function-at-discrete-points-or-equivalently-as-the-expected-value-of-the-function-f-over-a-distribution-of-sets-encoded-by-mathbfx&quot;&gt;What is a scalar set function extension? It is simply a new differentiable function \(\mathfrak{F}: [0,1]^n \rightarrow \mathbb{R}\)  which extends the discrete domain of the original function to a continuous domain. We will define it as a weighted combination of evaluations of the original function at discrete points, or equivalently, as the expected value of the function \(f\) over a distribution of sets encoded by \(\mathbf{x}\).&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/images/extensions/pencil_noise.png&quot; alt=&quot;image description&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That means that we will be going from a function defined on the corners of the hypercube \(\{0,1\}^n\) to a function defined on the whole hypercube \([0,1]^n\). We propose continuous and differentiable extensions of discrete functions defined on sets, that can be &lt;em&gt;deterministically&lt;/em&gt; computed, even when we’re only given black-box access to \(f\). We provide multiple extensions that originate from a common mathematical formulation. Some are already known like the Lovász extension, and others are new, like the bounded cardinality Lovász extension.&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;scalar-extensions-explained&quot;&gt;Scalar extensions explained&lt;/h2&gt;
&lt;p&gt;We call  \(\mathfrak{F}\) a scalar extension of a function \(f\), if  \(\mathfrak{F}(\mathbf{1}_S) = f(S)\) for all \(S\) in the domain of \(f\). For example,  \(\mathfrak{F}(\mathbf{0}) = f(\emptyset)\), i.e., the extension evaluated at the origin (all-zeros vector) should correspond to \(f\) being evaluated at the empty set.&lt;/p&gt;

&lt;h3 id=&quot;how-can-you-compute--mathfrakf&quot;&gt;How can you compute  \(\mathfrak{F}\)?&lt;/h3&gt;
&lt;p&gt;There is a simple trick to defining \(\mathfrak{F}\). As explained earlier, we can view \(\mathbf{x}\) as encoding a distribution over sets.
 To do this, we can express any continuous point in the n-hypercube \(\mathbf{x} \in [0,1]^n\) in the following way:&lt;/p&gt;
&lt;h4 id=&quot;displaystyle--mathbfx--sum_i-a_i-mathbf1_s_i-quad-displaystyle-sum_i-a_i--1-quad-a_i-geq-0&quot;&gt;\(\displaystyle  \mathbf{x} = \sum_{i} a_i \mathbf{1}_{S_i}, \quad \displaystyle \sum_i a_i = 1, \quad a_i \geq 0.\)&lt;/h4&gt;
&lt;p&gt;In other words, for every continuous point in the hypercube, we can find certain corners of the hypercube and express said point as a convex combination of those corners. This defines a distribution:&lt;/p&gt;
&lt;h4 id=&quot;mathbfx---displaystylemathbbe_s-sim-mathbfp_xs&quot;&gt;\(\mathbf{x} =  \displaystyle\mathbb{E}_{S \sim \mathbf{p_x}}[S]\).&lt;/h4&gt;
&lt;p&gt;Here \(\mathbf{p_x}\) is the probability distribution induced by \(\mathbf{x}\). This distribution is fully described by the coefficients \(a_i\) in the convex combination.
Once we have \(S_i\) and \(a_i\) defining \(\mathfrak{F}\)  is simple:&lt;/p&gt;
&lt;h4 id=&quot;mathfrakfmathbfx--triangleq-displaystylesum_i-a_i-fs_i--mathbbe_s-sim-mathbfp_xfs&quot;&gt;\(\mathfrak{F}(\mathbf{x})  \triangleq \displaystyle\sum_{i} a_i f({S_i}) = \mathbb{E}_{S \sim \mathbf{p_x}}[f(S)]\).&lt;/h4&gt;
&lt;p&gt;The distribution is supported only on sets \(S_i\) and each set has a corresponding probability of \(a_i\).
Intuitively, the value of the extension at the continuous point is just a weighted combination of the values of the original function \(f\) at the corners that form the convex combination of \(\mathbf{x}\). Furthermore, the weights of the weighted combination are precisely the same weights used to express \(\mathbf{x}\) as a convex combination of hypercube corners.&lt;/p&gt;

&lt;p&gt;A large chunk of the paper is dedicated to properly formalizing this trick. Obviously, I will keep things simple here so I won’t get into all that. I encourage the curious reader to check out the paper.
Now, there are a couple of questions that naturally have to be addressed. One has to do with how we find those sets \(S_i\) and their coefficients \(a_i\). Is it computationally tractable? Can we do it fast? 
The other has to do with whether this thing is differentiable at all. 
The answer to all those questions is, modulo certain disclaimers, yes. (Note that by ‘differentiable’ I will only be referring to differentiability in the sense that we can use automatic differentiation with packages like Pytorch and Tensorflow. Check out the appendix of the paper for more detailed comments around that.)&lt;/p&gt;

&lt;h3 id=&quot;how-can-we-backpropagate-through-mathfrakfmathbfx&quot;&gt;How can we backpropagate through \(\mathfrak{F}(\mathbf{x})\)?&lt;/h3&gt;
&lt;p&gt;The trick for this is simple. As long as \(a_i = g(\mathbf{x})\), and \(g\) is some differentiable function of \(\mathbf{x}\), then gradients can just go through
the coefficients \(a_i\) in the sum that defines \(\mathfrak{F}(\mathbf{x})\). (The coefficients)
&lt;img src=&quot;/images/extensions/extensions_pencil2.png&quot; alt=&quot;image description&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;how-do-we-find-sets-s_i-and-their-coefficients-a_i&quot;&gt;How do we find sets \(S_i\) and their coefficients \(a_i\)?&lt;/h3&gt;
&lt;p&gt;In the paper we provide multiple examples of scalar extensions. Each one comes with its own way of computing sets \(S_i\) and their probabilities \(a_i\). Crucially, the coefficients \(a_i\) depend continuously on \(\mathbf{x}\) which allows us to do backpropagation.  The ‘cheapest’ extensions that require only black-box access to the function \(f\) are the Lovász extension, the bounded-cardinality Lovász extension, the singleton, and the permutation/involutory extensions.
These all require \(n+1\) sets (including the empty set) and coefficients.  Before I go into specific extensions and how to compute them, I want to emphasize that the key point to remember is that you could find your own extensions by figuring out ways to express continuous points as convex combinations of discrete points. I may discuss some general strategies for this in a future post but for now I will just leave it at that.&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;in-practice-the-lovász-extension&quot;&gt;In practice: The Lovász extension&lt;/h2&gt;
&lt;p&gt;The Lovász extension is well known in the fields of discrete analysis/optimization and submodularity. It has various particularly nice properties but perhaps the most important one is that iff \(f\) is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Submodular_set_function&quot;&gt;submodular function&lt;/a&gt;, then the Lovász extension of \(f\) is convex. Examples of submodular functions include graph cuts, coverage functions, rank functions, and so on. 
Computing the Lovász extension is straightforward.
First, we index the entries of \(\mathbf{x}\) in sorted, decreasing order: \(x_i \geq x_{i+1}\) for \(i=1,2,\dots , n-1\). That means that \(x_1\) corresponds to the dimension with the largest entry in \(\mathbf{x}\), \(x_2\) to the second largest, and so on.
The coefficients and the sets of the Lovász extension are then defined as follows:&lt;/p&gt;
&lt;h3 id=&quot;a_i--x_i---x_i1&quot;&gt;\(a_i = x_i - x_{i+1}\)&lt;/h3&gt;
&lt;h3 id=&quot;s_i--1i-&quot;&gt;\(S_i = \{1:i \}\).&lt;/h3&gt;
&lt;p&gt;Here, I’m using a bit of coding notation with \(1:i\) to indicate “all elements up to \(i\)”.
Given the sets and their coefficients, the Lovász extension is computed by&lt;/p&gt;
&lt;h3 id=&quot;mathfrakfmathbfx--displaystyle-sum_i1n--x_i---x_i1f-1-i--&quot;&gt;\(\mathfrak{F}(\mathbf{x}) = \displaystyle \sum_{i=1}^{n}  (x_i - x_{i+1})f( \{1: i \} )\).&lt;/h3&gt;
&lt;p&gt;Clearly, \(a_i\) are differentiable with respect to \(\mathbf{x}\) as they’re just pairwise differences of the coordinates of \(\mathbf{x}\) 
To make things concrete, let’s do the calculation to find sets and coefficients for our example vector from before: \(\mathbf{x} = \begin{bmatrix} 0.3 &amp;amp; 0.5 &amp;amp; 0.2\end{bmatrix}\).
Based on the ranking of the elements, we will have the following sets&lt;/p&gt;
&lt;h3 id=&quot;1_s_1--beginbmatrix-0--1--0-endbmatrix-1_s_2--beginbmatrix-1--1--0-endbmatrix-1_s_3--beginbmatrix-1--1--1-endbmatrix&quot;&gt;\(1_{S_1} = \begin{bmatrix} 0 &amp;amp; 1 &amp;amp; 0 \end{bmatrix}, 1_{S_2} = \begin{bmatrix} 1 &amp;amp; 1 &amp;amp; 0 \end{bmatrix}, 1_{S_3} = \begin{bmatrix} 1 &amp;amp; 1 &amp;amp; 1 \end{bmatrix}\),&lt;/h3&gt;
&lt;p&gt;and the following coefficients&lt;/p&gt;
&lt;h3 id=&quot;a_1--05-03--02&quot;&gt;\(a_1 = 0.5-0.3 = 0.2,\)&lt;/h3&gt;
&lt;h3 id=&quot;a_2--03-02--01&quot;&gt;\(a_2 = 0.3-0.2 = 0.1,\)&lt;/h3&gt;
&lt;h3 id=&quot;a_3--02-&quot;&gt;\(a_3 = 0.2\) .&lt;/h3&gt;
&lt;p&gt;It is easy to verify that \(\mathbf{x} = \sum_{i} a_i \mathbf{1}_{S_i}\). One might notice that \(\sum_i a_i  &amp;lt;1\) in this case, even though I initially said that we need a sum to one. Thankfully, that’s not a problem because we can allocate the remaining probability mass to the empty set. By convention \(f(\emptyset) = 0\) so that term just cancels out. Therefore, we don’t strictly need the sum to 1, \(\sum_i a_i \leq 1\) can also be fine.&lt;/p&gt;

&lt;h2 id=&quot;converting-discrete-objectives-to-loss-functions-for-combinatorial-optimization-and-beyond&quot;&gt;Converting discrete objectives to loss functions for combinatorial optimization and beyond&lt;/h2&gt;
&lt;p&gt;A standard use case for extensions in practice is when we have some kind of quantity of interest that is defined over sets of objects and we would like to find a set that optimizes it. This is a sufficiently general setting that could encompass graph and combinatorial problems, classification problems, NLP problems, etc. Extensions can let us do that by finding the continuous version of that quantity through \(\mathfrak{F}\) and then backpropagating to find representations that optimize it.&lt;/p&gt;

&lt;h2 id=&quot;combinatorial-objectives&quot;&gt;Combinatorial objectives&lt;/h2&gt;
&lt;p&gt;Let’s look at a simple application of extensions to combinatorial optimization. Consider the graph cut function. It is a submodular set function, that given a set \(S\) of nodes in an input graph \(G\), it counts the number of edges that have one endpoint inside \(S\) and one endpoint outside of \(S\) in the graph. It is known that finding a set that maximizes the graph cut is an NP-Hard problem. Minimizing it is by default in P (can you guess why?) but adding a simple cardinality constraint can make minimization NP-Hard as well. A way to tackle the problem of optimizing the cut (either min or max) could be to provide a graph instance on \(n\) nodes to a neural network and then generate some scores for the nodes of the graph \(\mathbf{x} \in [0,1]^n\). That’s where the extensions come in. Here we can use the extension of the graph cut function as a loss function to optimize. Then we can backpropagate through the neural network scores \(\mathbf{x}\) and update the parameters of the network. Normally, that wouldn’t be possible because we wouldn’t be able to backpropagate through an arbitrary set function. 
Now, the graph cut function happens to admit numerous continuous relaxations that are well known. One of those is the graph total variation \(TV(\mathbf{x};G) = \displaystyle\sum_{(x_i&amp;gt;x_j)} (x_i-x_j)w_{i,j}\), where \(w_{i,j}\) is the weight for any edge \((i,j)\) in the graph \(G\). It turns out that the Lovász Extension of the graph cut function is precisely the TV function so this is a case where a bespoke relaxation is naturally absorbed in our framework.&lt;/p&gt;

&lt;p&gt;However, the same trick could be done for any other combinatorial problem. Take its objective function \(g(S)\) and a function that encodes the constraint \(c(S)\). For instance, for the maximum clique problem, \(g(S)\) is the number of nodes of \(S\), which we seek to maximize. The constraint \(c(S)\) on the other hand can just be a binary signal, it can be defined as follows:&lt;/p&gt;
&lt;h3 id=&quot;cs--begincases-1--textif-the-set-is-a-clique--0--text-otherwise-endcases&quot;&gt;\(c(S) = \begin{cases} 1 \; \text{if the set is a clique,} \\ 0 \; \text{ otherwise.} \end{cases}\)&lt;/h3&gt;
&lt;p&gt;Then we may combine \(c\) and \(g\) into \(f(S) = c(S)g(S)\). This is now a discrete set function which we can attempt to maximize in order to solve the maximum clique problem. We compute its continuous extension and use that as a loss function in order to find a score vector \(\mathbf{x}\) that encodes a distribution of sets \(S\) that best solves the problem.&lt;/p&gt;

&lt;h2 id=&quot;putting-it-all-together-a-recipe-for-solving-problems-with-extensions&quot;&gt;Putting it all together: A recipe for solving problems with extensions&lt;/h2&gt;
&lt;p&gt;To summarize, here are 4 steps to start solving problems with extensions. Assuming you have some discrete function \(f: \{0,1\}^n \rightarrow \mathbb{R}\) and you are looking for subsets of \(n\) objects that give you the optimal value for that function.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Step 1: Get a neural network and an input instance (image, graph, whatever).&lt;/li&gt;
  &lt;li&gt;Step 2: Generate scores \(\mathbf{x} \in [0,1]^n\). The dimension \(n\) may be the number of nodes of a graph, the number of classes for classification, etc.&lt;/li&gt;
  &lt;li&gt;Step 3: Compute an extension \(\mathfrak{F}\) of the function \(f\). Use \(\mathfrak{F}\) (with an appropriate sign for minimization/maximization) as your loss function.&lt;/li&gt;
  &lt;li&gt;Step 4: To decide on a set for the solution to the problem, generate the sets \(S\) of the extension and pick whichever gives you the best value for \(f\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That’s pretty much it. In the paper we apply those steps to do combinatorial optimization but we also show how they can be used for image classification by defining an extension for the discrete training error function (the possible input subsets there are just the \(n\) possible class labels that are represented by one-hot binary vectors). From brief discussions I’ve had 
at NeurIPS 2022, it seems like this kind of trick could be applied to other settings like NLP or VAEs. Those applications are left as an exercise to the reader :)&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;conclusion-and-part-2-neural-extensions&quot;&gt;Conclusion and Part 2: Neural Extensions&lt;/h2&gt;
&lt;p&gt;To conclude, apart from using extensions as loss functions, we may incorporate them at other parts of end-to-end pipelines. For example, we could compute  structural attributes of node neighborhoods in a graph using set functions, and then use those attributes as node features for a GNN that solves some downstream task. Extensions would allow us to backpropagate through the computation of those input attributes. I am obviously speculating here, but the bottom line is that apart from being used as loss functions, extensions could serve other purposes in an end-to-end pipeline.&lt;/p&gt;

&lt;p&gt;Finally, this post has dealt with the first half of the paper. You may have noticed that I avoided discussing the meaning of the word &lt;em&gt;scalar&lt;/em&gt; in the subtitle of the post. The characterization of &lt;em&gt;scalar&lt;/em&gt; set function extensions has to do with the domain of the extension. If the extensions work on the hypercube like \(\{0,1\}^n \rightarrow  [0,1]^n\), we are assigning a single scalar continuous score to each item among \(n\) items. In part 2 of this series, we will see how we can stretch the definition of extensions to allow for extensions of the form \(\{0,1\}^n \rightarrow  [0,1]^{n \times d}\), i.e., \(d\)-dimensional embeddings for each item, which is usually the kind of representation that the layers of a neural network operate with (\(d\) would just be the width of a NN). 
But that’s a story for another time. Until then, I hope this has been helpful.&lt;/p&gt;</content><author><name></name></author><category term="posts" /><category term="Learning with discrete functions" /><category term="Neural Networks" /><category term="Combinatorial optimization" /><summary type="html">This post is meant to serve as an intuitive and practical explainer of our work on neural set function extensions, recently presented at NeurIPS 2022. As in previous posts, I will be skipping potentially important details in the interest of brevity and accessibility. Throughout the text, I will be making an effort to accommodate the non-expert reader. This might make the text a bit tedious or repetitive for the people already familiar with this kind of setting, so feel free to skim through the boring parts if that’s you. Now, let’s get started.</summary></entry><entry><title type="html">Erdos Goes Neural</title><link href="http://localhost:4000/posts/2022-01-05/Erdos_Goes_Neural.html" rel="alternate" type="text/html" title="Erdos Goes Neural" /><published>2022-01-05T00:00:00-05:00</published><updated>2022-01-05T00:00:00-05:00</updated><id>http://localhost:4000/posts/2022-01-05/Erdos_Goes_Neural</id><content type="html" xml:base="http://localhost:4000/posts/2022-01-05/Erdos_Goes_Neural.html">&lt;h3 id=&quot;disclaimer&quot;&gt;Disclaimer:&lt;/h3&gt;
&lt;p&gt;This is a brief tutorial on the probabilistic penalty method presented in our &lt;a href=&quot;https://arxiv.org/abs/2006.10643&quot;&gt;paper&lt;/a&gt;. I will be taking shortcuts in how I present it in order to make it straightforward and understandable. It does not cover the paper in its full generality. I will not explain the probabilistic method and I won’t expand on the various important details that constitute the full paper.
Instead, I aim to provide an accessible primer into our work by minimizing the technical info and focusing on practicality/simplicity. I encourage you to read the paper and the supplementary material section for the complete treatment. Otherwise, if you’re still unsure, just email me. Without further ado, let’s get started.&lt;/p&gt;

&lt;h2 id=&quot;erdos-goes-neural-a-simplified-tutorial&quot;&gt;Erdos goes neural: a simplified tutorial&lt;/h2&gt;

&lt;h3 id=&quot;preliminaries&quot;&gt;Preliminaries:&lt;/h3&gt;
&lt;p&gt;Before we start, we should clear up what the goal of this work is. The main objective is to solve combinatorial problems with neural networks, 
&lt;strong&gt;without supervision&lt;/strong&gt; (i.e., without access to labeled solutions). We focused on graph problems but the principles of our method can be applied to other settings. Furthermore, many (combinatorial) problems have an equivalent graph formulation, so you could reformulate your problem in a graph setting if you want to be as close to what we did as possible.&lt;/p&gt;

&lt;p&gt;OK, great. Now that this is out of the way, let’s talk about how we are going to get things done.
First, let’s be a bit more specific with the setup.&lt;/p&gt;

&lt;p&gt;We want a set of objects \(S\) that solves a given problem, e.g., finds the set of nodes that forms the largest possible clique on a graph (maximum clique problem). 
Since we focused on graph problems in the paper, we will work with sets of nodes on the input graph, but you can also do sets of edges/tuples/etc. It will depend on your particular problem and what is more convenient for you. 
&lt;strong&gt;From now on I will be talking about sets of nodes. Feel free to substitute it with edges or whatever else you need to work with.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here is what we are going to need:&lt;/p&gt;
&lt;h4 id=&quot;1-a-combinatorial-problem-with-a-nonnegative-cost-function-and-a-set-of-constraints&quot;&gt;1) A combinatorial problem with a nonnegative cost function and a set of constraints.&lt;/h4&gt;
&lt;h4 id=&quot;2-a-neural-network-that-produces-a-probability-for-each-node-this-is-the-probability-that-the-node-belongs-to-the-set-s&quot;&gt;2) A neural network that produces a probability for each node. This is the probability that the node belongs to the set \(S\).&lt;/h4&gt;
&lt;h4 id=&quot;3-a-differentiable-loss-function-this-loss-takes-as-input-the-probabilities-that-were-produced-by-the-network-the-loss-will-be-derived-from-your-problems-objective-i-will-explain-below&quot;&gt;3) A differentiable loss function. This loss takes as input the probabilities that were produced by the network. The loss will be derived from your problem’s objective (I will explain below).&lt;/h4&gt;

&lt;h4 id=&quot;technical-detail&quot;&gt;Technical Detail:&lt;/h4&gt;
&lt;p&gt;In the paper, we use Bernoulli variables over the nodes of the input graph. You could work with other distributions as well, but if you are unsure I recommend starting with Bernoulli  random variables over the entities you care about (over nodes/edges/tuples/etc.). We will need to derive an expectation of a function over those random variables and it happens that Bernoulli variables tend to lead to easier derivations.
For the rest of the post \(x_i\) is a Bernoulli random variable placed on node \(i\),&lt;/p&gt;

\[x_i= \begin{cases} 1, \quad \text{with probability } p_i  \\
0, \quad \text{with probability } 1-p_i .
\end{cases}\]

&lt;h2 id=&quot;how-to-solve-it&quot;&gt;How to solve it&lt;/h2&gt;

&lt;p&gt;Now, suppose you have a combinatorial problem. You need to follow the steps below.&lt;/p&gt;
&lt;h3 id=&quot;step-1&quot;&gt;Step 1:&lt;/h3&gt;
&lt;p&gt;Write down the cost function of your problem, for example in our paper we consider the maximum clique problem.
Here is the standard way of expressing it:&lt;/p&gt;
&lt;h4 id=&quot;maximize-weights-subject-to-s-is-a-clique&quot;&gt;maximize weight(\(S\)), subject to: \(S\) is a clique.&lt;/h4&gt;
&lt;p&gt;For a simple undirected graph, weight(S) (henceforth \(w(S)\)) just counts the number of edges in the subgraph induced by S. 
To be in line with the paper, we need to switch to a  minimization problem, hence:&lt;/p&gt;
&lt;h4 id=&quot;minimize-gamma-ws-subject-to-s-is-a-clique&quot;&gt;minimize \(\gamma-w(S)\), subject to: \(S\) is a clique.&lt;/h4&gt;
&lt;p&gt;\(\gamma\) here is a sufficiently large constant;  Let \(\gamma \geq \text{max}_{S}(w(S))\), so that the expression remains always nonnegative.&lt;/p&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;step-2&quot;&gt;Step 2:&lt;/h3&gt;
&lt;p&gt;Set up a graph neural network (the choice of model depends on the task and engineering considerations) for your data. Using any of the mainstream
GNNs (GIN, GAT, etc.) should be fine to start with.
The GNN takes as input some node features. Its output is an N x 1 vector of probabilities, one for each node.
For the specifics on features, layers, normalizations, etc. you can just look at the code in the repo. Or you may improve upon the pipeline by working with your own features, layers, etc.&lt;/p&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;step-3&quot;&gt;Step 3:&lt;/h3&gt;
&lt;p&gt;Derive a differentiable loss.
The differentiable loss function has to look like this:&lt;/p&gt;
&lt;h4 id=&quot;loss--expected-cost--beta--ps-does-not-satisfy-constraints&quot;&gt;Loss = Expected Cost + \(\beta\) * P(S does not satisfy constraints).&lt;/h4&gt;
&lt;p&gt;\(\beta\) here is a coefficient that controls the importance of the constraint in the loss. 
The expectation of the cost can be straightforward for many set functions. 
For the Prob(S does not satisfy constraints)  term (henceforth \(P(S \notin \Omega)\)), we can use Markov’s inequality to bound it.&lt;/p&gt;

&lt;h3 id=&quot;example&quot;&gt;Example:&lt;/h3&gt;
&lt;p&gt;First, derive the expected cost. In our max-clique example, \(\text{cost} = \gamma-w(S)\).
\(\gamma\) is just a constant so \(E[cost] = \gamma - E[w(S)]\).
We have \(w(S) = \sum_{(i,j) \in E}x_i x_j\) where \(E\) is the set of edges of the graph.&lt;/p&gt;

&lt;p&gt;In other words, an edge \((i,j)\) is in  \(S\) if both endpoints \(i\) and \(j\) are in  \(S\).
We are using Bernoulli variables, therefore:
\(E[w(S)] = \sum_{(i,j)\in E}p_i p_j.\)&lt;/p&gt;

&lt;p&gt;OK, now onto the constraint part of the loss.
For the maximum clique problem, the constraint dictates that &lt;strong&gt;the subgraph induced by S is a clique&lt;/strong&gt;, i.e, all pairs of nodes in S are connected by an edge.
An equivalent way to phrase this is that, &lt;strong&gt;there are no edges in the complement of the subgraph induced by S&lt;/strong&gt;.
Let \(\bar{S}\) be the complement of S. Then,&lt;/p&gt;
&lt;h4 id=&quot;ps-notin-omega---pwbars1&quot;&gt;\(P(S \notin \Omega)  = P(w(\bar{S})&amp;gt;=1).\)&lt;/h4&gt;
&lt;p&gt;From Markov’s inequality, this can be bounded as follows&lt;/p&gt;
&lt;h4 id=&quot;pwbars1-leq-ewbars&quot;&gt;\(P(w(\bar{S})&amp;gt;=1) \leq E[w(\bar{S})].\)&lt;/h4&gt;
&lt;p&gt;So our loss ends up being&lt;/p&gt;
&lt;h4 id=&quot;textloss--gamma---ews--beta-ewbars&quot;&gt;\(\text{Loss} = \gamma - E[w(S)] + \beta E[w(\bar{S})].\)&lt;/h4&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;step-4&quot;&gt;Step 4:&lt;/h3&gt;
&lt;p&gt;Train the network using the derived loss. This is straightforward because you just have to plug in the probabilities in the expression and do backprop.&lt;/p&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;step-5&quot;&gt;Step 5:&lt;/h3&gt;
&lt;p&gt;Retrieve the set \(S\) from the probabilities of the network using the method of conditional expectation.
It works as follows.&lt;/p&gt;

&lt;p&gt;Sort the nodes according to their probabilities.
Starting from the high probability nodes, for each node \(v_i\) do:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Evaluate the loss for \(p_i=1\) and for \(p_i=0\).&lt;/li&gt;
  &lt;li&gt;Set  \(p_i\) to either 1 or 0 depending on what achieved the better loss.&lt;/li&gt;
  &lt;li&gt;Move on to the next node and repeat from step 1.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When this is done, you should have a binary (indicator) vector that represents your set \(S\), which is the solution to your problem.
Congrats! That’s it, you’re done.&lt;/p&gt;</content><author><name></name></author><category term="posts" /><category term="Erdős goes neural" /><category term="Combinatorial Optimization" /><category term="Neural Networks" /><summary type="html">Disclaimer: This is a brief tutorial on the probabilistic penalty method presented in our paper. I will be taking shortcuts in how I present it in order to make it straightforward and understandable. It does not cover the paper in its full generality. I will not explain the probabilistic method and I won’t expand on the various important details that constitute the full paper. Instead, I aim to provide an accessible primer into our work by minimizing the technical info and focusing on practicality/simplicity. I encourage you to read the paper and the supplementary material section for the complete treatment. Otherwise, if you’re still unsure, just email me. Without further ado, let’s get started.</summary></entry><entry><title type="html">World Spirit and the Matrix Revolutions</title><link href="http://localhost:4000/posts/2021-12-30/The-Matrix-Dialectics.html" rel="alternate" type="text/html" title="World Spirit and the Matrix Revolutions" /><published>2021-12-30T00:00:00-05:00</published><updated>2021-12-30T00:00:00-05:00</updated><id>http://localhost:4000/posts/2021-12-30/The%20Matrix%20Dialectics</id><content type="html" xml:base="http://localhost:4000/posts/2021-12-30/The-Matrix-Dialectics.html">&lt;p&gt;The Matrix franchise has been in the spotlight for the past few months due to the release of the fourth installment “The Matrix: Resurrections”. The discussions I see online have reminded me how misunderstood the sequels are. It’s no secret that I love the franchise. It’s absolutely packed with symbolism and cool ideas that over the years keep impressing me with how bold and ambitious they are. In this post, I will focus on one of the key concepts that makes its appearance in the third film: the golden light. &lt;a href=&quot;https://i.imgur.com/eNO90eB.jpg&quot;&gt;&lt;img src=&quot;https://i.imgur.com/eNO90eB.jpg&quot; alt=&quot;Machine City&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As a quick refresher, starting from the end of the second film, we can see that Neo is able to “feel” the sentinels and even shut them down at will. He is also destroying sentinels en masse on his way to the machine city while Trinity is driving, even though he can’t even see them. People have argued that he has somehow gained access to a kind of wireless connection to the machines through his interaction with the source in the second film. I don’t think this holds up as an explanation because in the third film, he is blinded by Bane (who is possessed by Smith) and we can clearly see that he is able to see Smith in the form of golden light inside Bane’s flesh even though he’s blind. There are no mechanical parts in Bane (well, except for the holes I suppose), his consciousness has just been altered by Smith. Furthermore, he can perceive the entire machine city as formations of golden light. Whatever effect the source had on Neo seems to transcend ordinary electronic circuits.&lt;/p&gt;

&lt;p&gt;Below I will provide a more philosophical (and speculative) account of what the golden light is and how it connects to Neo’s abilities based on material that has been released either with the movies or after them.
If you expect a strictly mechanistic explanation of everything you will probably be disappointed. This is more of a loose attempt at putting things together and as such, we probably need to tone down the scientist in us a little bit. There are certain interpretative jumps that need to be taken which (in my view) are quite consistent with information that is presented in the movies and other media from the Wachowskis themselves.&lt;/p&gt;

&lt;p&gt;So… let’s get the obvious stuff out of the way. Apart from pure curiosity, what makes the golden light worth discussing? 
From the philosopher’s commentary with Dr. Cornel West and Ken Wilber as well as the comments by Lana Wachowski in &lt;a href=&quot;https://www.matrixfans.net/symbolism-philosophy-and-allegory/the-many-meanings-of-the-matrix/&quot;&gt;a discussion between Lana and Ken Wilber&lt;/a&gt;,
it becomes clear that the gold light that Neo perceives in the city of Machines has major conceptual significance for the trilogy.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Ken Wilber: As you know, I think it’s incredibly gutsy because the whole key
to the Matrix trilogy—this is my interpretation—is given in really in
the last fifteen, twenty minutes of the third film; that the Rosetta
Stone is when Neo, for example, is saying of the machines, “If you
could only see them like I see em…they’re all light.  They’re made
of light”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is just Ken Wilber’s interpretation though. However, he does point out that he has “corroborated” his interpretation with the Wachowskis to some extent. Here’s what Lana had to say
in an &lt;a href=&quot;https://youtu.be/ARoKJ00cEZ8?t=4246&quot;&gt;interview&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;If you basically work on what the trilogy is all about, you come to this symbol that is never discussed… and yet it’s there clearly. The third movie actually begins with the gold light as a part of the opening title montage. As if to say that…look, this is an essential element. It’s as important as the matrix code is.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;My central claim in this post is that the golden light is meant to symbolize &lt;span style=&quot;color:rgb(255, 166, 0)&quot;&gt;spirit&lt;/span&gt; 
. 
I will do my best to explain what that means and why this view makes sense. Additionally, I’ll provide you with sources that support this claim and I’ll briefly go over them to describe what each source suggests.&lt;/p&gt;

&lt;h2 style=&quot;color:rgb(255, 166, 0); font-family:Calibri;font-size: 22px;font-weight:100;&quot;&gt;Opening Sequence&lt;/h2&gt;

&lt;p&gt;The golden light appears in the opening sequence of the Matrix Revolutions. In the youtube video I linked, Lana explains that the opening sequence tries to provide a visual “summary” of what the film is about. You can watch the sequence &lt;a href=&quot;https://youtu.be/cYEcjGi_kv0?t=42&quot;&gt;on this link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The sequence begins with an explosion of golden light symbolizing the Big Bang, and an emergence of fractal patterns which are most likely a rendering of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Julia_set&quot;&gt;Julia set&lt;/a&gt;. As the camera keeps zooming inside the fractals, it eventually reaches a depiction of the machine city. Fractals exhibit self-similarity and (roughly speaking) are produced by an iterative process that converges for certain points and diverges for others. Fractals are rather generic entities that have been used in all sorts of metaphors so by themselves they’re not enough to draw any solid conclusion. Following the fractals we zoom out to matrix code and a panoramic view of the matrix Megacity which looks like a symbol. So the Matrix and the Megacity appear to be layered &lt;em&gt;on top&lt;/em&gt; of the golden light in this sequence.&lt;/p&gt;

&lt;p&gt;Given Lana’s comments and that the golden light is shown to underlie the matrix and the Megacity, it is fair to assume at this point that it’s a fundamental layer of the story, and given the Big Bang depiction, of reality as presented in the matrix.
How do we know it’s supposed to be the Big Bang and not some flashy random explosion? Well, here’s the quote from Lana in the discussion with Ken Wilber:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;That was the…  in the beginning of the third movie when there’s like… we’re like: “How do we start the third movie? Which is gonna talk about the things that are so hard to talk about?”  It’s like: Ok, you go to black and then you have to have a moment of Big Bang and that’s the origin of everything, the origin of thought, the origin of consciousness, whatever it is—in that moment it’s like ‘from that nothing to everything’ is everything… [Ken laughs]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Given the title of the movie and the emphasis of the franchise on repeated patterns (for example the cycle of Zion being “rebooted” by the machines through the One), it can be fair to connect the fractal to the concept of “Revolutions”, in the sense of repeating processes that revolve around a certain point. The fractals can be viewed as a loose visual metaphor for said revolutions. Furthermore, the self-similarity may be viewed as reflecting the different layers of the story that share similar characteristics. For example, the power struggle between Neo and Smith in the Matrix can be seen to reflect the larger struggle of humans and machines. The underlying fabric of those cycles and power struggles is the golden light, i.e., spirit. But what does that even mean? This is still all very vague, so let’s get into it.&lt;/p&gt;

&lt;p&gt;To explain in more detail we will need to introduce some key concepts that will be essential to understanding those connections.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: I am not a philosopher and I have read about those things only in online lecture material, blog posts, and online encyclopedias. I’m sure there will be people that find this too crude, misguided, or even outright incorrect. If there are points in this post where what I’m saying is blatantly wrong, feel free to shoot me an email to let me know :)&lt;/p&gt;

&lt;p&gt;What I’m presenting here is just my reading of the material and more particularly, how I am guessing that the Wachowskis may have understood and interpreted this material to connect it to the Matrix. Of course, I don’t know them and I’ve never talked to them so that’s just me piecing things together in a way that hopefully makes sense.&lt;/p&gt;

&lt;h2 style=&quot;color:rgb(255, 166, 0); font-family:Calibri;font-size: 22px;font-weight:100;&quot;&gt;Spirit&lt;/h2&gt;

&lt;p&gt;So, what is spirit?
There are multiple accounts of what constitutes spirit. One of the most widely discussed realizations of the concept comes from the philosopher &lt;a href=&quot;https://en.wikipedia.org/wiki/Georg_Wilhelm_Friedrich_Hegel&quot;&gt;Georg Wilhelm Friedrich Hegel
&lt;/a&gt;. In the article by Rex Martin “The World Spirit” we read:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The main speculative claim of Hegelian philosophy is that the totality
of human history is a single whole and the work of one Spirit (a World
Spirit or World-Historical Spirit), i.e., of human activity taken en
bloc, as a single collectivity. The entire history of the world is to
be viewed as a single “life”, as the career of one self or person.
World History is a self-organizing process. What happens in World
History is the progressive actualization of the only truly individual
person or self (the World Spirit).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So, the bottom line is that the history of the world is meant to be viewed as the journey of self-actualization of spirit. To a non-philosopher like myself this sounds
like a borderline religious claim, but there’s more. Spirit is meant to be perceived almost as a different state of matter. It is not localized in individuals, minds, or objects.
It is meant to inhabit another realm where the “collective-consciousness” of an entire society resides. In some sense, you can view spirit as encoding fundamental progress towards
a greater self-awareness (often referred to as “self-consciousness”) of the universe.&lt;/p&gt;

&lt;p&gt;Here’s another quote from Lana regarding Hegel during the discussion with Ken Wilber:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;It’s the whole process, that mystical—that Eros that you talk about, that’s underneath everything, has been bringing us toward the development of self-awareness and consciousness.  Well I guess consciousness, and then self-awareness.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;But how is this progress towards universal “self-consciousness” achieved?
Before we get into that, check out this quote about the trilogy from a recent &lt;a href=&quot;https://youtu.be/LQYGbuxYgGg?t=314&quot;&gt;interview&lt;/a&gt; that Lana Wachowski did:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“We wrote it as an elegant structure that was &lt;strong&gt;dialectical&lt;/strong&gt; in nature… It was resonant with ideas of birth, life, death… and &lt;strong&gt;thesis,&lt;/strong&gt;
&lt;strong&gt;antithesis, synthesis…&lt;/strong&gt;”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Interestingly, Hegel postulated that spirit  evolves in a dialectical manner. What does that mean? First, let’s understand the other concept from the quote.&lt;/p&gt;

&lt;h2 style=&quot;color:rgb(255, 166, 0); font-family:Lato;font-size: 22px;font-weight:100;&quot;&gt;Thesis, Antithesis, Synthesis&lt;/h2&gt;

&lt;p&gt;From the Stanford encyclopedia of philosophy:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;This “textbook” Being-Nothing-Becoming example is closely connected to
the traditional idea that Hegel’s dialectics follows a
thesis-antithesis-synthesis pattern, which, when applied to the logic,
means that one concept is introduced as a “thesis” or positive
concept, which then develops into a second concept that negates or is
opposed to the first or is its “antithesis”, which in turn leads to a
third concept, the “synthesis”, that unifies the first two.
…
Being is the positive moment or thesis, Nothing is the negative moment or antithesis, and Becoming is the moment of aufheben or synthesis—the concept that cancels and preserves, or unifies and combines, Being and Nothing.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thesis-Antithesis-Synthesis can be used to describe the evolution of a logical argument. This is what Lana means when she says that the structure was dialectical. We start with a proposition (thesis). Then a contradiction in that proposition leads to a counter-proposition (antithesis).
The two are reconciled by creating a new proposition.
Neo’s struggle with Smith can be metaphorically viewed in those terms.
Essentially, starting from a thesis (Neo) a contradiction arises which leads
to the creation of antithesis (Smith). How the contradiction (and therefore Smith) emerged is handwaved away in Reloaded. All we get is that there’s something teleological about his existence (“it happened for a reason”). These two have to be reconciled through synthesis; a new entity that combines the previous ones. This is the moment where Neo gets absorbed by Smith and through Deus-Ex-Machina achieves peace.&lt;/p&gt;

&lt;h2 style=&quot;color:rgb(255, 166, 0); font-family:Calibri;font-size: 22px;font-weight:100;&quot;&gt;Master-Slave Dialectic&lt;/h2&gt;

&lt;p&gt;Now back into the evolution of spirit.
Hegel in “The Phenomenology of Spirit” explained that consciousness of individuals culminates into spirit through interactions with other individual consciousnesses. This happens
when one consciousness is able to &lt;em&gt;recognize&lt;/em&gt; another consciousness and place it on equal footing with itself. This occurs in history through the Master-Slave dialectic. In
the article “Hegel’s Master-Slave Dialectic: the search for self-consciousness” by J.D. Feilmeier we read:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Hegel’s Master-Slave dialectic tells the story of two independent “self-consciousnesses” who encounter one another and engage in a life-and-death struggle. The two self-consciousnesses must struggle because each one sees the other as a threat to itself. Until the confrontation, each self-consciousness has seen itself as the measure of all things. Its feelings, desires, powers, etc. have been the objective standard by which all things encountered have been measured. Now, however, the presence of another self-consciousness establishes a new objective standard -the feelings, desires, and powers of each self-consciousness are subjective standards which must be measured against the new objective standards – the feelings, desires, and powers of the other. This affirmation of self-consciousness requires a struggle to the death because each self-consciousness can only become aware of Its limits by exerting itself to a maximum effort. Each self-consciousness must struggle with all its might in order to realize the extent of its strength in relation to the other.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;OK, we can sort of see a parallel here between Neo and Smith (humans and machines) and those two “self-consciousnesses”. Neo stands for &lt;strong&gt;choice and free will&lt;/strong&gt; while Smith represents &lt;strong&gt;fate and purpose&lt;/strong&gt;. They represent radically conflicting worldviews and they do engage in a death struggle. The connection is still kind of loose though, so we keep reading:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The significance of its existence which each individual fought so fiercely to establish is fully realized by the slave rather than the master. In winning the struggle, the master has affirmed his power over the slave, yet the master has failed to recognize that he is not god. The slave, however, recognizes his limitedness, slavery to the master “Is his chain, from which he could not, in the struggle, get away, and for that reason he proves himself dependent, shows that his independence consists in his being a thing.” (Hegel, p.405) Only in being subdued and forced to see himself as merely an object, is it possible for the slave to become fully aware of his place in the universe.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Therefore, it is essential for the slave to completely submit in order gain awareness.
This keeps getting closer to what happened in the film. At the end of Revolutions, Neo is completely vulnerable in front of both Smith and Deus-Ex-Machina.
Neo had to concede to Smith and let himself be absorbed. Not only does he physically surrender, but on another level we also have free will submitting to fate by having Neo accept the inevitability of what will happen. “You were always right. It was inevitable”. That allowed Deus-Ex-Machina to use Neo as a way to liberate the Matrix from Smith.
&lt;a href=&quot;https://i.imgur.com/SexHdvh.jpg&quot;&gt;&lt;img src=&quot;https://i.imgur.com/SexHdvh.jpg&quot; alt=&quot;You were right&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now, let’s take a quick look at the wiki for the &lt;a href=&quot;https://en.wikipedia.org/wiki/Master%E2%80%93slave_dialectic&quot;&gt;Master-Slave dialectic&lt;/a&gt;. We read:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Crucially, for Hegel, absolute knowledge (Spirit) cannot come to be without first a self-consciousness recognizing another self-consciousness. He maintained that the entire reality is immediately present to self-consciousness. It undergoes three stages of development:
&lt;strong&gt;Desire&lt;/strong&gt;, where self-consciousness is directed at things other than itself.
&lt;strong&gt;Master-slave&lt;/strong&gt;, where the self-consciousness is directed at another that is unequal to itself.
&lt;strong&gt;Universal self-conscious&lt;/strong&gt;, where the self-conscious recognizes itself in another.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;And a few lines below:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;A struggle to the death ensues. However, if one of the two should die, the achievement of self-consciousness fails. Hegel refers to this failure as “abstract negation” not the negation or sublation required. This death is avoided by the agreement, communication of, or subordination to, slavery.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thematically, it appears that Neo’s role was essentially the role of achieving self-consciousness by engaging in a death struggle with Smith and ultimately surrendering. In those moments, we see the shots of golden light completely overtaking Neo. At that point we’re seeing the emergence of Spirit. &lt;a href=&quot;https://i.stack.imgur.com/8cUUh.jpg&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/8cUUh.jpg&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You could argue that I’m reaching quite a bit to get this interpretation so let’s bring some more evidence to back this up.
The most important clue is the final level of the game “The Matrix: Path of Neo”.
&lt;a href=&quot;https://www.youtube.com/watch?v=Fgg7FdznyQg&quot;&gt;Here&lt;/a&gt; is the excerpt where Lana (as Larry at the time) discusses the final scene of Revolutions:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;You see, at this point in the story, Neo stands on the verge of
satori, ready to resolve the paradox of choice and choicelessness, of
free will versus fate, but that can only be achieved through an act of
surrender, which occurs after he has abandoned the perspectival nature
of truth, accepting the totality of present consciousness which
ultimately allows an evolutionary transition, transcending the
Cartesian dilemma through the emergence of delimited  &lt;span style=&quot;color:rgb(255, 166, 0)&quot;&gt;spirit&lt;/span&gt;, which
then provides the world with the choice of a third path, the path of
Neo, the path of peace.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;OK, this seems familiar, doesn’t it? I will focus on one line for the sake of brevity and the rest can be more or less understood with a little bit of extra effort.
“…accepting the totality of present consciousness which ultimately allows an evolutionary transition, transcending the Cartesian dilemma through the emergence of delimited spirit”.&lt;/p&gt;

&lt;p&gt;By Cartesian dilemma, Lana refers to the mind-body duality as it manifests through the Matrix and Zion. In the trilogy, the world of the body is the world of Zion and the city of Machines. There, everything is colored in blue tones. The Matrix represents the world of the mind and is always colored in green tones (Ken Wilber highlights this repeatedly in the philosopher’s commentary versions of the movies). Then Neo, by acknowledging Smith’s perspective (“you were right”) and surrendering leads to the emergence of spirit (golden light) and is able to reconcile the worlds of body and mind (the Matrix returns back to its normal state).&lt;/p&gt;

&lt;p&gt;As Neo ends the war and people in Zion realize that the war is over, the track playing in the background is called &lt;span style=&quot;color:rgb(255, 166, 0)&quot;&gt;“Spirit of the Universe”&lt;/span&gt;, composed by Don Davis.
Furthermore, the significance of the blue and green colors makes a subtle appearance in that last sequence. This is how the city of the machines looks like right before Neo engages in the final battle:
&lt;a href=&quot;https://i.imgur.com/d9HgE04.jpg&quot;&gt;&lt;img src=&quot;https://i.imgur.com/d9HgE04.jpg&quot; alt=&quot;machine city&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After Neo succeeds, the lights in the machine city are beginning to turn green:
&lt;a href=&quot;https://i.imgur.com/joX8nXr.jpg&quot;&gt;&lt;img src=&quot;https://i.imgur.com/joX8nXr.jpg&quot; alt=&quot;machine city&quot; /&gt;&lt;/a&gt;
Given Lana’s words about transcending the Cartesian dilemma, it’s fair to assume that this is a deliberate choice, meant to signify the unification of the worlds of mind and body.&lt;/p&gt;

&lt;h2 style=&quot;color:rgb(255, 166, 0); font-family:Lato;font-size: 22px;font-weight:100;&quot;&gt;The Big Picture&lt;/h2&gt;

&lt;p&gt;So, why does it even make sense to go there? What is the point of invoking all those philosophical ideas? Well, apart from serving as great literary devices, we need to remember where the Matrix started from.
Starting with the first, the main character is forced to completely revise his framework for understanding the world as he realizes that he was trapped in a simulated reality. Upon re-evaluating his worldview and assuming the role of the one, he realizes that this new framework has to be re-evaluated as well, as it’s yet another system of control laid out by the machines. Naturally, by the end of Reloaded we may wonder whether there are further reveals down the line.
 That’s where Revolutions comes. It aims to provide an account of how a kind of “higher awareness” (spirit of the universe) emerges during the confrontation of entities with conflicting views and categorizations of the world. Ultimately, it seems like there is a value judgment, i.e., there is a preferable course of events which involves resolving the conflict (the path of peace) and that is achieved when Neo understands the role he has to play.&lt;/p&gt;

&lt;p&gt;Lana in her interview says about the golden light:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;That light is what the third movie is trying to solve, post deconstructing something like choice. You can say choice is important, choice is a part of everyone’s lives… but that paradigm of choice is not different, it’s a matrix just like Catholicism or Christianity…
…a tool for understanding the world, having a framework of meaning.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At the grander level of this narrative, we have humans developing AI which reflects a new kind of consciousness. This type of consciousness has a completely mechanistic view of the world, and its value system
revolves around the ability to mathematically account for everything. Their power to model the world and predict outcomes makes them view the world as predetermined. This clashes with the worldview of humans
which stand for the values of choice and free will. A master-slave dialectic between the humans and machines emerges. As machines shape the world through servitude to humans, they become independent from humans. From Hegel’s ‘Philosophy of World History’:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;“The humankind has not liberated itself from servitude but by means of servitude.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Interestingly, this is flipped and applies to machines in the trilogy. Now, in this master-slave struggle machines gain the upper hand. In Reloaded, we learn that the Architect can not comprehend humans. He epitomizes the values of machines that are diametrically opposed to the human ones. The Oracle also explicitly tells Neo that the Architect can’t understand humans.  However, we see that the Oracle begins to gain an appreciation for human nature. As we saw above, spirit emerges through an act of mutual recognition between conflicting sides. That is, the values of the machines and the values of humans would somehow have to be reconciled. This manifests with the Oracle’s plan that creates a new version of the One. Recall that previous matrix versions failed because humans were not given a (subconscious) choice to participate. This solution of incorporating choice was discovered by the Oracle.&lt;/p&gt;

&lt;p&gt;The recognition of the values of the machines comes through Neo’s acknowledgement of inevitability and his surrender. From the machine side, I believe that this acknowledgement is best reflected in the last line of the film. Seraph asks the Oracle if she always knew what would happen. Her response is ‘No. But I believed’. We see that the Oracle’s actions clearly display values that are typically representative of humans (choice, hope, and belief). First, she was able to realize how choice is integral for humans. Then, she herself embraces the human ‘framework of meaning’. This is what allowed spirit to emerge and the peaceful coexistence of humans and machines to be achieved.&lt;/p&gt;

&lt;p&gt;And this is why I find Revolutions to be the most ambitious sequel. It provids a bold and idealistic view for the evolution of history in terms of the evolution of spirit. I’m not particularly spiritual in my life, but spiritual themes can be great at driving narratives in movies and this is what appears to be happening here. While those ideas are not always explicitly detailed in the movie, this is in part because the Wachowskis intended this to be a puzzle for people to analyze. 
The pieces of the puzzle are there, although arguably not entirely contained in the trilogy. It’s quite likely that some of the connections I drew here are half-baked and more crisp explanations may exist. I do think that the &lt;a href=&quot;https://www.youtube.com/watch?v=ObpcGNCU944&quot;&gt;g(e)ist&lt;/a&gt; of what I’m describing might be getting fairly close to what the Wachowskis intended.
I have also left out parts like the religious references of the trilogy. 
I may come back to write a few things about how those ideas relate to spirit, but for now… it is done.&lt;/p&gt;

&lt;p&gt;I will just end this post with a quote from Hegel’s “Philosophy of Nature” which I find rather fitting:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;All cultural change reduces itself to a difference of categories. All revolutions, whether in the sciences or world history, occur merely because spirit has changed its categories in order to understand and examine what belongs to it, in order to possess and grasp itself in a truer, deeper, more intimate and unified manner.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;iframe width=&quot;800&quot; height=&quot;480&quot; src=&quot;https://www.youtube.com/embed/dqXTGSK_iBo&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content><author><name></name></author><category term="posts" /><category term="The Matrix" /><category term="Spirit of the Universe" /><summary type="html">The Matrix franchise has been in the spotlight for the past few months due to the release of the fourth installment “The Matrix: Resurrections”. The discussions I see online have reminded me how misunderstood the sequels are. It’s no secret that I love the franchise. It’s absolutely packed with symbolism and cool ideas that over the years keep impressing me with how bold and ambitious they are. In this post, I will focus on one of the key concepts that makes its appearance in the third film: the golden light.</summary></entry></feed>